---
title: "Tidy and Wrangle Data for ggplot2"
output: 
  html_document:
    theme: readable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(eval = FALSE)
```

The data that was used for the [Intro to ggplot2](https://psrc.github.io/intro-ggplot2/) module had already been cleaned and transformed ahead of time. While we didn't spend time on the details of that process, it shouldn't be overlooked. The code below documents that process. Having tidy data is an important step when working with ggplot2. By cleaning and transforming first, you will save yourself many lines of code and prevent hardcoding, especially if the dataset is updated on a frequent basis with new additional data each time.

The original data source can be downloaded [here](https://www.ofm.wa.gov/sites/default/files/public/dataresearch/pop/april1/ofm_april1_population_final.xlsx) from the Office of Financial Management (OFM). 

To run this code on your own machine:

- copy the code below into a new R script file
- set `my.dir` to a directory of your choice
- store the original data file in that directory
- Run line by line to see how the data is transformed

For more information about tidy data, see the *Tidying Messy Datasets* section in [this summary](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html).

```{r}
# This script uses base R syntax to read-in OFM April 1 Total Population data and transforms it to be used with ggplot2

library(openxlsx) # reads excel file
library(reshape2) # helps pivot data
library(stringr) # tools to extract parts of strings using regular expressions
library(lubridate) # tools to convert data into various date-time formats

my.dir <- "C:/Users/clam/Documents/github/intro-ggplot2"

# read in excel data
raw <- read.xlsx(file.path(my.dir, "ofm_april1_population_final.xlsx"), startRow = 4)

# filter for PSRC region and remove Line column
region <- subset(raw, County %in% c("King", "Kitsap", "Pierce", "Snohomish"))
keep.cols <- setdiff(colnames(region), "Line")
region <- region[, keep.cols]

# OFM publishes data in a cross-tab (wider) format 
# pivot the table longer so that we have a new column called 'melted_cols'
# to contain those column headers with information such as year, the data attribute (population), and data source type
id.cols <- c("Filter", "County", "Jurisdiction")
df <- melt(region, id.vars = id.cols, variable.name = "melted_cols", value.name = "Estimate")

# create separate columns for data year (20##), attribute (Population), and source (Census or Estimate)
df$Year_chr <- str_extract(df$melted_cols, "^\\d+") # year as character datatype
df$Year_dt <- ymd(as.numeric(df$Year_chr), truncated = 2L) # year as a formal date (in case we need it)
df$Attribute <- str_extract(df$melted_cols, "(?:[^\\d+\\.])\\w+")
df$Source <- str_extract(df$melted_cols, "\\w+$")

# exclude melted_cols and re-order columns
cols.order <- c(id.cols, "Attribute", "Source", "Year_chr", "Year_dt", "Estimate")
df <- df[, cols.order]

# checkout the structure of df
str(df)

# the Estimate column should contain numeric values, not characters!
# convert the Estimates to be actual numbers and double check the structure
df$Estimate <- as.numeric(df$Estimate)
str(df)

# export table
write.xlsx(df, file.path(my.dir, "ofm_april1_population_final_demo.xlsx"))
```

